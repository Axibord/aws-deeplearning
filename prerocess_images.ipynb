{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and dimensionality reduction using Pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark # module for BigData preprocessing\n",
    "# import boto3 # Module for interaction with AWS services(EC2, S3)\n",
    "import time, os, io, sys\n",
    "import boto3, cv2, pandas\n",
    "import numpy as np\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise s3 session with credentials (Not mandatory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paire de cl√©s dans la console IAM sur AWS\n",
    "session = boto3.session.Session(aws_access_key_id=\"AKIA4DUZXJ5W3AS4EX63\",\n",
    "                                aws_secret_access_key=\"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\")\n",
    "\n",
    "s3_client = session.client(service_name=\"s3\", region_name=\"eu-central-1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions d'automatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  download folders from AWS s3\n",
    "def downloadDirectoryFroms3(bucketName,remoteDirectoryName):\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    bucket = s3_resource.Bucket(bucketName) \n",
    "    for object in bucket.objects.filter(Prefix = remoteDirectoryName):\n",
    "        if not os.path.exists(os.path.dirname(object.key)):\n",
    "            os.makedirs(os.path.dirname(object.key))\n",
    "        bucket.download_file(object.key,object.key)\n",
    "        \n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "def read_image(img_path):\n",
    "    \"\"\"load images with Pillow and extract the\n",
    "\n",
    "    Arguments:\n",
    "        img: image path\n",
    "\n",
    "    Return\n",
    "    --------\n",
    "    Return vector in list format\n",
    "    \"\"\"\n",
    "\n",
    "    # start timer\n",
    "    timer_start = time.time()\n",
    "    \n",
    "    \n",
    "#     full_path = img_path.split(\"/\")\n",
    "#     image_path = full_path[-1]\n",
    "    print(img_path)\n",
    "    \n",
    "    # open the image\n",
    "    image = Image.open(img_path)\n",
    "\n",
    "    if image is None:\n",
    "        image = 0\n",
    "    else:\n",
    "        image = np.asarray(image)\n",
    "        image = image.flatten().tolist()\n",
    "\n",
    "    print(\"Executed in: {} seconds\".format(time.time() - timer_start))\n",
    "\n",
    "    return image\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# folder_to_load should end with '/' i think\n",
    "\n",
    "def load_data(folder_to_load, spark, sc):\n",
    "    \"\"\"\n",
    "    function that obtain full path of images in all subdirectories  and save them into Spark\n",
    "    Dataframe\n",
    "    Parameters\n",
    "    --------\n",
    "    spark : Spark Session\n",
    "    sc: Spark Context\n",
    "    return\n",
    "    --------\n",
    "    Return Spark DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # start timer\n",
    "    timer_start = time.time()\n",
    "    \n",
    "    images_path = []\n",
    "    \n",
    "    # list of sub-directories (represent labels in our case)\n",
    "    sub_folders = os.listdir(folder_to_load)\n",
    "    print(sub_folders)\n",
    "    \n",
    "    # get full paths of images\n",
    "    for file in sub_folders:\n",
    "        label = os.listdir(folder_to_load + file)\n",
    "        \n",
    "        # get images in each sub-folder\n",
    "        for img in label:\n",
    "            if (len(images_path)) < 100: \n",
    "                images_path.append(folder_to_load + file + \"/\" + img)\n",
    "            else:\n",
    "                break\n",
    "             \n",
    "            \n",
    "            \n",
    "    print(images_path)\n",
    "    print(\"Numbers of images found\",len(images_path))\n",
    "    \n",
    "    # Transform paths into Spark Dataframe\n",
    "    rdd = sc.parallelize(images_path)\n",
    "    row_rdd = rdd.map(lambda x:Row(x))\n",
    "    df = spark.createDataFrame(row_rdd, [\"image_path\"])\n",
    "    \n",
    "    # print execution time\n",
    "    print(\"Execution time: {} seconds\".format(time.time() - timer_start))\n",
    "    \n",
    "    return df\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "def extract_labels(path):\n",
    "    \"\"\"\n",
    "    Extract the label of an image\n",
    "\n",
    "    Return\n",
    "    --------\n",
    "    Return string\n",
    "    \"\"\"\n",
    "    path_file = path.split(\"/\")\n",
    "\n",
    "    label = path_file[-2]\n",
    "    print(\"Execution finished\")\n",
    "\n",
    "    return label\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "def dim_reduc(img):\n",
    "    \"\"\"extract descriptors of each image using OpenCv\n",
    "\n",
    "    Return\n",
    "    --------\n",
    "    Return vector in list format\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    image = Image.open(img)\n",
    "    image = np.asarray(image)\n",
    "\n",
    "    # start timer\n",
    "    start_timer = time.time()\n",
    "\n",
    "    orb = cv2.ORB_create(nfeatures=50)\n",
    "    orb_key_points, descriptors = orb.detectAndCompute(image, None)\n",
    "\n",
    "    if descriptors is None:\n",
    "        descriptors = 0\n",
    "    else:\n",
    "        descriptors = descriptors.flatten().tolist()\n",
    "\n",
    "    print(\"Executed in: {} seconds\".format(time.time() - start_timer))\n",
    "\n",
    "    return descriptors\n",
    "#----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download files from AWS s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axibord-fruits\n",
      "elasticbeanstalk-eu-central-1-832472305517\n"
     ]
    }
   ],
   "source": [
    "# tell what service to use\n",
    "s3= boto3.resource('s3')\n",
    "\n",
    "# print all available buckets\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloadDirectoryFroms3('axibord-fruits', remoteDirectoryName='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engine running\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    sc.setLogLevel(\"WARN\")\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"name\").getOrCreate()\n",
    "    print(\"Engine running\")\n",
    "except:\n",
    "    print(\"Error when building Spark Engine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nectarine', 'Apple Golden 2', 'Papaya', 'Cucumber Ripe', 'Melon Piel de Sapo', 'Mangostan', 'Grape White 2', 'Kaki', 'Pear', 'Granadilla', 'Lemon', 'Blueberry', 'Pepper Red', 'Potato White', 'Apple Red Yellow 2', 'Grape Blue', 'Pear Red', 'Apple Red 2', 'Avocado ripe', 'Grape White 3', 'Pineapple Mini', 'Quince', 'Nectarine Flat', 'Cantaloupe 2', 'Onion Red', 'Pomegranate', 'Pear Stone', 'Cantaloupe 1', 'Limes', 'Cherry Rainier', 'Clementine', 'Kiwi', 'Apple Golden 1', 'Apple Red 1', 'Maracuja', 'Peach Flat', 'Grape Pink', 'Beetroot', 'Physalis with Husk', 'Pear Forelle', 'Nut Forest', 'Peach 2', 'Dates', 'Guava', 'Grapefruit White', 'Peach', 'Pear Abate', 'Onion Red Peeled', 'Pineapple', 'Mango', 'Cherry Wax Black', 'Apple Pink Lady', 'Avocado', 'Apple Red Yellow 1', 'Lychee', 'Hazelnut', 'Nut Pecan', 'Mango Red', 'Cactus fruit', 'Banana Red', 'Pepino', 'Ginger Root', 'Pear Kaiser', 'Cocos', 'Cauliflower', 'Carambula', 'Pear Monster', 'Apple Braeburn', 'Apple Granny Smith', 'Cherry Wax Red', 'Pomelo Sweetie', 'Corn Husk', 'Pepper Green', 'Pepper Yellow', 'Cucumber Ripe 2', 'Fig', 'Banana Lady Finger', 'Banana', 'Physalis', 'Apple Crimson Snow', 'Apple Red 3', 'Chestnut', 'Mandarine', 'Grapefruit Pink', 'Plum', 'Plum 3', 'Passion Fruit', 'Apple Golden 3', 'Grape White', 'Kumquats', 'Grape White 4', 'Lemon Meyer', 'Potato Red Washed', 'Pear Williams', 'Apricot', 'Corn', 'Kohlrabi', 'Onion White', 'Apple Red Delicious', 'Pear 2', 'Pitahaya Red', 'Mulberry', 'Eggplant', 'Cherry Wax Yellow', 'Plum 2', 'Huckleberry', 'Cherry 1', 'Orange', 'Cherry 2', 'Pepper Orange', 'Potato Red', 'Potato Sweet']\n",
      "['Test/Nectarine/r_55_100.jpg', 'Test/Nectarine/r_37_100.jpg', 'Test/Nectarine/33_100.jpg', 'Test/Nectarine/r_324_100.jpg', 'Test/Nectarine/45_100.jpg', 'Test/Nectarine/44_100.jpg', 'Test/Nectarine/r_86_100.jpg', 'Test/Nectarine/6_100.jpg', 'Test/Nectarine/r_62_100.jpg', 'Test/Nectarine/48_100.jpg', 'Test/Nectarine/r_77_100.jpg', 'Test/Nectarine/r_44_100.jpg', 'Test/Nectarine/40_100.jpg', 'Test/Nectarine/4_100.jpg', 'Test/Nectarine/63_100.jpg', 'Test/Nectarine/r_84_100.jpg', 'Test/Nectarine/96_100.jpg', 'Test/Nectarine/r_35_100.jpg', 'Test/Nectarine/r_73_100.jpg', 'Test/Nectarine/r_76_100.jpg', 'Test/Nectarine/94_100.jpg', 'Test/Nectarine/r_68_100.jpg', 'Test/Nectarine/41_100.jpg', 'Test/Nectarine/68_100.jpg', 'Test/Nectarine/r_46_100.jpg', 'Test/Nectarine/r_87_100.jpg', 'Test/Nectarine/r_40_100.jpg', 'Test/Nectarine/r_42_100.jpg', 'Test/Nectarine/r_99_100.jpg', 'Test/Nectarine/r_52_100.jpg', 'Test/Nectarine/r_69_100.jpg', 'Test/Nectarine/r_60_100.jpg', 'Test/Nectarine/r_43_100.jpg', 'Test/Nectarine/r_96_100.jpg', 'Test/Nectarine/327_100.jpg', 'Test/Nectarine/36_100.jpg', 'Test/Nectarine/r_65_100.jpg', 'Test/Nectarine/92_100.jpg', 'Test/Nectarine/35_100.jpg', 'Test/Nectarine/54_100.jpg', 'Test/Nectarine/r_49_100.jpg', 'Test/Nectarine/323_100.jpg', 'Test/Nectarine/51_100.jpg', 'Test/Nectarine/49_100.jpg', 'Test/Nectarine/42_100.jpg', 'Test/Nectarine/70_100.jpg', 'Test/Nectarine/r_58_100.jpg', 'Test/Nectarine/325_100.jpg', 'Test/Nectarine/73_100.jpg', 'Test/Nectarine/38_100.jpg', 'Test/Nectarine/85_100.jpg', 'Test/Nectarine/r_4_100.jpg', 'Test/Nectarine/r_323_100.jpg', 'Test/Nectarine/r_90_100.jpg', 'Test/Nectarine/r_74_100.jpg', 'Test/Nectarine/60_100.jpg', 'Test/Nectarine/37_100.jpg', 'Test/Nectarine/71_100.jpg', 'Test/Nectarine/r_33_100.jpg', 'Test/Nectarine/r_95_100.jpg', 'Test/Nectarine/32_100.jpg', 'Test/Nectarine/90_100.jpg', 'Test/Nectarine/r_92_100.jpg', 'Test/Nectarine/59_100.jpg', 'Test/Nectarine/r_326_100.jpg', 'Test/Nectarine/r_47_100.jpg', 'Test/Nectarine/r_59_100.jpg', 'Test/Nectarine/r_5_100.jpg', 'Test/Nectarine/84_100.jpg', 'Test/Nectarine/324_100.jpg', 'Test/Nectarine/r_36_100.jpg', 'Test/Nectarine/9_100.jpg', 'Test/Nectarine/93_100.jpg', 'Test/Nectarine/r_71_100.jpg', 'Test/Nectarine/39_100.jpg', 'Test/Nectarine/r_327_100.jpg', 'Test/Nectarine/65_100.jpg', 'Test/Nectarine/r_63_100.jpg', 'Test/Nectarine/r_48_100.jpg', 'Test/Nectarine/r_67_100.jpg', 'Test/Nectarine/5_100.jpg', 'Test/Nectarine/r_93_100.jpg', 'Test/Nectarine/89_100.jpg', 'Test/Nectarine/r_50_100.jpg', 'Test/Nectarine/83_100.jpg', 'Test/Nectarine/r_98_100.jpg', 'Test/Nectarine/58_100.jpg', 'Test/Nectarine/7_100.jpg', 'Test/Nectarine/r_6_100.jpg', 'Test/Nectarine/95_100.jpg', 'Test/Nectarine/r_38_100.jpg', 'Test/Nectarine/r_82_100.jpg', 'Test/Nectarine/r_72_100.jpg', 'Test/Nectarine/r_7_100.jpg', 'Test/Nectarine/r_325_100.jpg', 'Test/Nectarine/r_53_100.jpg', 'Test/Nectarine/87_100.jpg', 'Test/Nectarine/86_100.jpg', 'Test/Nectarine/47_100.jpg', 'Test/Nectarine/77_100.jpg']\n",
      "Numbers of images found 100\n",
      "Execution time: 3.710706949234009 seconds\n"
     ]
    }
   ],
   "source": [
    "spark_df = load_data('Test/', spark, sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|image_path                  |\n",
      "+----------------------------+\n",
      "|Test/Nectarine/r_55_100.jpg |\n",
      "|Test/Nectarine/r_37_100.jpg |\n",
      "|Test/Nectarine/33_100.jpg   |\n",
      "|Test/Nectarine/r_324_100.jpg|\n",
      "|Test/Nectarine/45_100.jpg   |\n",
      "+----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get images paths and categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting categories from images using UDF...\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting categories from images using UDF...\")\n",
    "\n",
    "udf_category = udf(extract_labels, StringType())\n",
    "spark_df = spark_df.withColumn(\"label\", udf_category(\"image_path\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|          image_path|    label|\n",
      "+--------------------+---------+\n",
      "|Test/Nectarine/r_...|Nectarine|\n",
      "|Test/Nectarine/r_...|Nectarine|\n",
      "|Test/Nectarine/33...|Nectarine|\n",
      "|Test/Nectarine/r_...|Nectarine|\n",
      "|Test/Nectarine/45...|Nectarine|\n",
      "+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create New Column of images vectors ( pixels ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading images...\")\n",
    "# create column using UDF that contain vector of each image\n",
    "udf_image = udf(read_image, ArrayType(IntegerType()))\n",
    "spark_df = spark_df.withColumn(\"image\", udf_image(\"image_path\"))\n",
    "spark_df = spark_df.filter(spark_df.image.isNotNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+\n",
      "|          image_path|    label|               image|\n",
      "+--------------------+---------+--------------------+\n",
      "|Test/Nectarine/r_...|Nectarine|[255, 255, 255, 2...|\n",
      "|Test/Nectarine/r_...|Nectarine|[255, 255, 255, 2...|\n",
      "|Test/Nectarine/33...|Nectarine|[255, 255, 255, 2...|\n",
      "|Test/Nectarine/r_...|Nectarine|[255, 255, 255, 2...|\n",
      "|Test/Nectarine/45...|Nectarine|[255, 255, 255, 2...|\n",
      "+--------------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demensionality reduction using ORB and adding it to Spark dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting descriptors using CV2\n",
      "Task finished.\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting descriptors using CV2\")\n",
    "udf_descriptors = udf(dim_reduc, ArrayType(IntegerType()))\n",
    "spark_df = spark_df.withColumn(\"descriptors\", udf_descriptors(\"image_path\"))\n",
    "spark_df = spark_df.filter(spark_df.descriptors.isNotNull())\n",
    "print(\"Task finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+--------------------+\n",
      "|          image_path|    label|               image|         descriptors|\n",
      "+--------------------+---------+--------------------+--------------------+\n",
      "|Test/Nectarine/r_...|Nectarine|[255, 255, 255, 2...|[16, 120, 123, 10...|\n",
      "|Test/Nectarine/r_...|Nectarine|[255, 255, 255, 2...|[254, 181, 168, 5...|\n",
      "|Test/Nectarine/33...|Nectarine|[255, 255, 255, 2...|[92, 183, 157, 11...|\n",
      "|Test/Nectarine/r_...|Nectarine|[255, 255, 255, 2...|[196, 237, 191, 2...|\n",
      "|Test/Nectarine/45...|Nectarine|[255, 255, 255, 2...|[220, 228, 157, 5...|\n",
      "+--------------------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Spark DataFrame to Pandas than save it locally to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.toPandas().to_csv('results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading results to AWS s3 in CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket('axibord-fruits').upload_file('results.csv', 'results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
